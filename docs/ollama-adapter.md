# Ollama Model Adapter

**Version:** 1.0

–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è –ø–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—é Ollama Model Adapter –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è LLM –≤ Arium.

---

## –û–≥–ª—è–¥

Ollama Adapter –¥–æ–∑–≤–æ–ª—è—î –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –ª–æ–∫–∞–ª—å–Ω—ñ LLM –º–æ–¥–µ–ª—ñ —á–µ—Ä–µ–∑ [Ollama](https://ollama.ai/) –±–µ–∑ –∑–∞–ª–µ–∂–Ω–æ—Å—Ç—ñ –≤—ñ–¥ –∑–æ–≤–Ω—ñ—à–Ω—ñ—Ö API.

### –ü–µ—Ä–µ–≤–∞–≥–∏

- ‚úÖ **Local-first** ‚Äî –ø—Ä–∞—Ü—é—î –ø–æ–≤–Ω—ñ—Å—Ç—é –ª–æ–∫–∞–ª—å–Ω–æ
- ‚úÖ **–ë–µ–∑–∫–æ—à—Ç–æ–≤–Ω–æ** ‚Äî –±–µ–∑ API costs
- ‚úÖ **–ü—Ä–∏–≤–∞—Ç–Ω—ñ—Å—Ç—å** ‚Äî –¥–∞–Ω—ñ –Ω–µ –ø–æ–∫–∏–¥–∞—é—Ç—å —Ç–≤—ñ–π –∫–æ–º–ø'—é—Ç–µ—Ä
- ‚úÖ **–®–≤–∏–¥–∫—ñ—Å—Ç—å** ‚Äî –±–µ–∑ –º–µ—Ä–µ–∂–µ–≤–∏—Ö –∑–∞—Ç—Ä–∏–º–æ–∫ (—è–∫—â–æ –º–æ–¥–µ–ª—å –ª–æ–∫–∞–ª—å–Ω–∞)
- ‚úÖ **–ì–Ω—É—á–∫—ñ—Å—Ç—å** ‚Äî –±–∞–≥–∞—Ç–æ –¥–æ—Å—Ç—É–ø–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π

---

## –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è Ollama

### macOS / Linux

```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

### Windows

–ó–∞–≤–∞–Ω—Ç–∞–∂ —ñ –≤—Å—Ç–∞–Ω–æ–≤–∏ –∑ [ollama.ai](https://ollama.ai/download)

### –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞

```bash
ollama --version
```

---

## –ó–∞–ø—É—Å–∫ Ollama

```bash
ollama serve
```

–¶–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å Ollama —Å–µ—Ä–≤–µ—Ä –Ω–∞ `http://localhost:11434`.

---

## –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª–µ–π

### –ü–æ–ø—É–ª—è—Ä–Ω—ñ –º–æ–¥–µ–ª—ñ

```bash
# Llama 2 (7B) - –Ω–∞–π–ø–æ–ø—É–ª—è—Ä–Ω—ñ—à–∞
ollama pull llama2

# Llama 2 (13B) - –±—ñ–ª—å—à–∞, –∫—Ä–∞—â–∞ —è–∫—ñ—Å—Ç—å
ollama pull llama2:13b

# Mistral
ollama pull mistral

# CodeLlama (–¥–ª—è –∫–æ–¥—É)
ollama pull codellama

# Phi-2 (Microsoft, –∫–æ–º–ø–∞–∫—Ç–Ω–∞)
ollama pull phi
```

### –°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π

```bash
ollama list
```

---

## –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è Arium

### Environment Variables

–î–æ–¥–∞–π —É `.env`:

```env
# Enable Ollama
USE_OLLAMA=true

# Ollama URL (optional, default: http://localhost:11434)
OLLAMA_URL=http://localhost:11434

# Model name (optional, default: llama2)
OLLAMA_MODEL=llama2
```

### –ü—Ä—ñ–æ—Ä–∏—Ç–µ—Ç –∞–¥–∞–ø—Ç–µ—Ä—ñ–≤

Arium –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –Ω–∞—Å—Ç—É–ø–Ω–∏–π –ø–æ—Ä—è–¥–æ–∫ –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç—ñ–≤:

1. **OpenAI** (—è–∫—â–æ `OPENAI_API_KEY` –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ)
2. **Ollama** (—è–∫—â–æ `USE_OLLAMA=true` –∞–±–æ `OLLAMA_URL` –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ)
3. **MockAdapter** (fallback –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è)

---

## –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è

### –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–µ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è

–Ø–∫—â–æ Ollama –¥–æ—Å—Ç—É–ø–Ω–∏–π, Arium –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –ø—ñ–¥–∫–ª—é—á–∏—Ç—å—Å—è:

```bash
USE_OLLAMA=true npm run dev
```

### –ü—Ä–æ–≥—Ä–∞–º–Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è

```typescript
import { OllamaAdapter } from "./core/models/ollamaAdapter";

const adapter = new OllamaAdapter({
  baseURL: "http://localhost:11434",
  model: "llama2",
});

// Check availability
const available = await adapter.isAvailable();
if (!available) {
  console.error("Ollama not available");
}

// List available models
const models = await adapter.listModels();
console.log("Available models:", models);

// Use with agent
const agent = new AgentCore({
  id: "local-agent",
  model: adapter,
}, eventBus, toolEngine);
```

---

## Tool Calling

Ollama –Ω–µ –º–∞—î –Ω–∞—Ç–∏–≤–Ω–æ—ó –ø—ñ–¥—Ç—Ä–∏–º–∫–∏ function calling, —Ç–æ–º—É Arium –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î prompt engineering:

1. Tools –æ–ø–∏—Å—É—é—Ç—å—Å—è —É prompt
2. –ú–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä—É—î —Å–ø–µ—Ü—ñ–∞–ª—å–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç: `CALL_TOOL: <name> <json>`
3. Adapter –ø–∞—Ä—Å–∏—Ç—å –≤—ñ–¥–ø–æ–≤—ñ–¥—å —ñ –≤–∏—Ç—è–≥—É—î tool call

### –ü—Ä–∏–∫–ª–∞–¥

```
Available tools:
- fs.read: Read file from VFS
  Parameters: {"path": "string"}
- fs.write: Write file to VFS
  Parameters: {"path": "string", "content": "string"}

To call a tool, respond with: CALL_TOOL: <tool_name> <json_arguments>
Example: CALL_TOOL: fs.read {"path": "file.txt"}

User: Read the file src/main.ts
Assistant: CALL_TOOL: fs.read {"path": "src/main.ts"}
```

---

## Streaming

Ollama adapter –ø—ñ–¥—Ç—Ä–∏–º—É—î streaming:

```typescript
const adapter = new OllamaAdapter({ model: "llama2" });

for await (const chunk of adapter.stream(prompt, { tools })) {
  if (chunk.type === "final") {
    console.log(chunk.content); // streaming text
  }
}
```

---

## –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è

### Temperature

```typescript
const adapter = new OllamaAdapter({
  model: "llama2",
});

const agent = new AgentCore({
  id: "agent",
  model: adapter,
  temperature: 0.7, // 0.0 = deterministic, 1.0 = creative
}, eventBus, toolEngine);
```

### Max Tokens

```typescript
const agent = new AgentCore({
  id: "agent",
  model: adapter,
  maxTokens: 4096,
}, eventBus, toolEngine);
```

---

## –ú–æ–¥–µ–ª—ñ —Ç–∞ –á—Ö–Ω—ñ –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏

### Llama 2 (7B)

- **–†–æ–∑–º—ñ—Ä**: ~4GB
- **–®–≤–∏–¥–∫—ñ—Å—Ç—å**: –®–≤–∏–¥–∫–∞
- **–Ø–∫—ñ—Å—Ç—å**: –î–æ–±—Ä–∞
- **–ü—Ä–∏–∑–Ω–∞—á–µ–Ω–Ω—è**: –ó–∞–≥–∞–ª—å–Ω—ñ –∑–∞–¥–∞—á—ñ

```bash
ollama pull llama2
```

### Llama 2 (13B)

- **–†–æ–∑–º—ñ—Ä**: ~7.5GB
- **–®–≤–∏–¥–∫—ñ—Å—Ç—å**: –°–µ—Ä–µ–¥–Ω—è
- **–Ø–∫—ñ—Å—Ç—å**: –ö—Ä–∞—â–∞ –∑–∞ 7B
- **–ü—Ä–∏–∑–Ω–∞—á–µ–Ω–Ω—è**: –°–∫–ª–∞–¥–Ω—ñ—à—ñ –∑–∞–¥–∞—á—ñ

```bash
ollama pull llama2:13b
```

### CodeLlama

- **–†–æ–∑–º—ñ—Ä**: ~4GB
- **–®–≤–∏–¥–∫—ñ—Å—Ç—å**: –®–≤–∏–¥–∫–∞
- **–Ø–∫—ñ—Å—Ç—å**: –í—ñ–¥–º—ñ–Ω–Ω–∞ –¥–ª—è –∫–æ–¥—É
- **–ü—Ä–∏–∑–Ω–∞—á–µ–Ω–Ω—è**: –ü—Ä–æ–≥—Ä–∞–º—É–≤–∞–Ω–Ω—è

```bash
ollama pull codellama
```

### Mistral

- **–†–æ–∑–º—ñ—Ä**: ~4GB
- **–®–≤–∏–¥–∫—ñ—Å—Ç—å**: –®–≤–∏–¥–∫–∞
- **–Ø–∫—ñ—Å—Ç—å**: –î—É–∂–µ —Ö–æ—Ä–æ—à–∞
- **–ü—Ä–∏–∑–Ω–∞—á–µ–Ω–Ω—è**: –ó–∞–≥–∞–ª—å–Ω—ñ –∑–∞–¥–∞—á—ñ, –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ Llama 2

```bash
ollama pull mistral
```

---

## Performance Tips

### –î–ª—è –∫—Ä–∞—â–æ—ó —à–≤–∏–¥–∫–æ—Å—Ç—ñ:

1. **–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –º–µ–Ω—à—ñ –º–æ–¥–µ–ª—ñ** (7B –∑–∞–º—ñ—Å—Ç—å 13B)
2. **–ó–º–µ–Ω—à–∏ max_tokens** —è–∫—â–æ –º–æ–∂–ª–∏–≤–æ
3. **GPU acceleration** (—è–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω–æ)
4. **–ö–µ—à—É–≤–∞–Ω–Ω—è** ‚Äî Ollama –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –∫–µ—à—É—î

### –î–ª—è –∫—Ä–∞—â–æ—ó —è–∫–æ—Å—Ç—ñ:

1. **–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π –±—ñ–ª—å—à—ñ –º–æ–¥–µ–ª—ñ** (13B, 34B)
2. **Fine-tuning** –º–æ–¥–µ–ª—ñ –ø—ñ–¥ —Ç–≤–æ—ó –∑–∞–¥–∞—á—ñ
3. **–ü—Ä–∞–≤–∏–ª—å–Ω—ñ prompts** ‚Äî –≤–∞–∂–ª–∏–≤—ñ—à–µ –Ω—ñ–∂ –¥–ª—è GPT

---

## Troubleshooting

### Ollama –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ

```bash
# –ü–µ—Ä–µ–≤—ñ—Ä —á–∏ Ollama –∑–∞–ø—É—â–µ–Ω–∏–π
curl http://localhost:11434/api/tags

# –ó–∞–ø—É—Å—Ç–∏ Ollama
ollama serve
```

### –ú–æ–¥–µ–ª—å –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞

```bash
# –°–ø–∏—Å–æ–∫ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π
ollama list

# –ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –º–æ–¥–µ–ª—å
ollama pull llama2
```

### –ü–æ–≤—ñ–ª—å–Ω–∞ —Ä–æ–±–æ—Ç–∞

- –ü–µ—Ä–µ–≤—ñ—Ä —â–æ GPU –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è (—è–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω–æ)
- –°–ø—Ä–æ–±—É–π –º–µ–Ω—à—É –º–æ–¥–µ–ª—å
- –ó–º–µ–Ω—à–∏ max_tokens

### –ü–æ–≥–∞–Ω–∞ —è–∫—ñ—Å—Ç—å –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π

- –°–ø—Ä–æ–±—É–π –±—ñ–ª—å—à—É –º–æ–¥–µ–ª—å (13B –∑–∞–º—ñ—Å—Ç—å 7B)
- –ù–∞–ª–∞—à—Ç—É–π temperature (0.7-0.9 –¥–ª—è –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—ñ)
- –ü–æ–∫—Ä–∞—â prompt engineering

---

## –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ OpenAI

| –§—É–Ω–∫—Ü—ñ—è | OpenAI | Ollama |
|---------|--------|--------|
| Cost | üí∞ –ü–ª–∞—Ç–Ω–∏–π | üÜì –ë–µ–∑–∫–æ—à—Ç–æ–≤–Ω–æ |
| Privacy | üåê Cloud | üè† Local |
| Speed | ‚ö° –®–≤–∏–¥–∫–∏–π | üê¢ –ó–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ HW |
| Quality | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| Function Calling | ‚úÖ Native | ‚ö†Ô∏è Prompt-based |
| Setup | ‚úÖ –ü—Ä–æ—Å—Ç–∏–π | ‚öôÔ∏è –ü–æ—Ç—Ä—ñ–±–Ω–æ –≤—Å—Ç–∞–Ω–æ–≤–∏—Ç–∏ |

---

## –ú–∞–π–±—É—Ç–Ω—ñ –ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è

- [ ] Native function calling (–∫–æ–ª–∏ Ollama –¥–æ–¥–∞—Å—Ç—å –ø—ñ–¥—Ç—Ä–∏–º–∫—É)
- [ ] Model switching –±–µ–∑ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫—É
- [ ] Automatic model selection
- [ ] GPU detection and optimization
- [ ] Model caching strategies
- [ ] Fine-tuning support

---

## –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó

### –î–ª—è Development

–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π **MockAdapter** –∞–±–æ –º–∞–ª–µ–Ω—å–∫—É –º–æ–¥–µ–ª—å (phi, llama2:7b) –¥–ª—è —à–≤–∏–¥–∫–æ—ó —ñ—Ç–µ—Ä–∞—Ü—ñ—ó.

### –î–ª—è Production

–ó–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ –ø–æ—Ç—Ä–µ–±:
- **Local-first**: Ollama –∑ –≤–µ–ª–∏–∫–æ—é –º–æ–¥–µ–ª–ª—é
- **Cloud-first**: OpenAI GPT-4
- **Hybrid**: Ollama –¥–ª—è –ø—Ä–æ—Å—Ç–∏—Ö –∑–∞–¥–∞—á, OpenAI –¥–ª—è —Å–∫–ª–∞–¥–Ω–∏—Ö

---

## –ü—Ä–∏–∫–ª–∞–¥–∏

### –ó–∞–ø—É—Å–∫ –∑ Ollama

```bash
# 1. –í—Å—Ç–∞–Ω–æ–≤–∏ Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# 2. –ó–∞–ø—É—Å—Ç–∏ Ollama
ollama serve

# 3. –ó–∞–≤–∞–Ω—Ç–∞–∂ –º–æ–¥–µ–ª—å
ollama pull llama2

# 4. –ù–∞–ª–∞—à—Ç—É–π .env
echo "USE_OLLAMA=true" >> .env
echo "OLLAMA_MODEL=llama2" >> .env

# 5. –ó–∞–ø—É—Å—Ç–∏ Arium
npm run dev
```

### –ü–µ—Ä–µ–º–∏–∫–∞–Ω–Ω—è –º—ñ–∂ –º–æ–¥–µ–ª—è–º–∏

```bash
# –£ .env
OLLAMA_MODEL=llama2:7b    # –î–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ
OLLAMA_MODEL=llama2:13b   # –î–ª—è —è–∫–æ—Å—Ç—ñ
OLLAMA_MODEL=codellama    # –î–ª—è –∫–æ–¥—É
```

---

